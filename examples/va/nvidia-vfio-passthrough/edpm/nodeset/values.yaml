# yamllint disable rule:line-length
# local-config: referenced, but not emitted by kustomize
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: edpm-nodeset-values
  annotations:
    config.kubernetes.io/local-config: "true"
data:
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to baremetalset-password-secret.data.NodeRootPassword
  root_password: cmVkaGF0Cg==
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to OpenStackDataPlaneNodeSet.spec.preProvisioned
  preProvisioned: false
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to BareMetalHost.metadata.annotations.inspect.metal3.io
  metal3_inspection: ""
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to BareMetalHost resource properties
  baremetalhosts:
    edpm-compute-0:
      online: true
      labels:
        nodeName: edpm-compute-0
      bmc:
        address: CHANGEME
        credentialsName: bmc-secret
        disableCertificateVerification: false
      bootMACAddress: CHANGEME
      rootDeviceHints:
        deviceName: /dev/vda
      preprovisioningNetworkDataName: edpm-compute-0-preprovision-network-data
      preprovisioningNetworkData:
        nmstate: |
          CHANGEME
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to edpm-compute-0-network-data and edpm-compute-1-network-data Secrets
  # Only provide it if it should not be auto-generated by EDPM operator
  baremetalHostsNetworkData:
    edpm-compute-0:
      networkData: |
        CHANGEME
  # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to OpenStackDataPlaneNodeSet.spec.baremetalSetTemplate
  baremetalSetTemplate:
    ctlplaneInterface: eno2  # CHANGEME
    cloudUserName: cloud-admin
    bmhNamespace: openstack
    # NOTE: provisioningInterface should not be provided when using virtual-media
    bmhLabelSelector:
      app: openstack  # CHANGEME
    passwordSecret:
      name: baremetalset-password-secret
      namespace: openstack
  # Mapped in lib/dataplane/nodeset/kustomization.yaml to the dataplane-ansible-ssh-private-key-secret
  ssh_keys:
    # Authorized keys that will have access to the dataplane computes via SSH
    authorized: CHANGEME
    # The private key that will have access to the dataplane computes via SSH
    private: CHANGEME2
    # The public key that will have access to the dataplane computes via SSH
    public: CHANGEME3
  nodeset:
    ansible:
      ansibleUser: cloud-admin
      ansiblePort: 22
      ansibleVars:
        # see https://access.redhat.com/solutions/253273
        edpm_bootstrap_command: |
          echo CHANGEME
        rhc_release: 9.4
        rhc_repositories:
          - {name: "*", state: disabled}
          - {name: "CHANGEME"}
        edpm_bootstrap_release_version_package: []
        timesync_ntp_servers:
          - hostname: pool.ntp.org
        # GPU passthrough settings for a PCI Vendor and Device ID pair.
        # Here, 10de represents NVIDIA Corporation, and 20f1 corresponds to the GA100 [A100 PCIe 40GB] GPU, according to Device Hunt
        # https://devicehunt.com/view/type/pci/vendor/10DE/device/20F1
        # TODO(bogdando): add noveau.blacklist=1 nvidia.blacklist=1 nvidia-drm.blacklist=1 nvidia-modeset.blacklist=1 nvidia-uvm.blacklist=1 rd.driver.blacklist=nouveau,nvidia,nvidia-drm,nvidia-modeset,nvidia-uvm ?
        edpm_kernel_args: "default_hugepagesz=1GB hugepagesz=1G hugepages=16 intel_iommu=on iommu=pt vfio-pci.ids=10de:20f1 rd.driver.pre=vfio-pci"
        edpm_reboot_strategy: never
        edpm_tuned_profile: "cpu-partitioning-powersave"
        edpm_tuned_isolated_cores: "4-23,28-47"
        # edpm_network_config
        # These vars are edpm_network_config role vars
        edpm_network_config_hide_sensitive_logs: false
        # Nics to MAC addresses mappings for the dataplane nodes
        edpm_network_config_os_net_config_mappings:
          edpm-compute-0:
            nic1: CHANGEME
            nic2: CHANGEME
        edpm_network_config_template: |
          ---
          # CHANGEME
          {% set mtu_list = [ctlplane_mtu] %}
          {% for network in nodeset_networks %}
          {{ mtu_list.append(lookup('vars', networks_lower[network] ~ '_mtu')) }}
          {%- endfor %}
          {% set min_viable_mtu = mtu_list | max %}
          network_config:
          - type: ovs_bridge
            name: {{ neutron_physical_bridge_name }}
            mtu: {{ min_viable_mtu }}
            use_dhcp: false
            dns_servers: {{ ctlplane_dns_nameservers }}
            domain: {{ dns_search_domains }}
            addresses:
            - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_cidr }}
            routes: {{ ctlplane_host_routes }}
            members:
            - type: interface
              name: nic2
              mtu: {{ min_viable_mtu }}
              # force the MAC address of the bridge to this interface
              primary: true
          {% for network in nodeset_networks %}
            - type: vlan
              mtu: {{ lookup('vars', networks_lower[network] ~ '_mtu') }}
              vlan_id: {{ lookup('vars', networks_lower[network] ~ '_vlan_id') }}
              addresses:
              - ip_netmask:
                  {{ lookup('vars', networks_lower[network] ~ '_ip') }}/{{ lookup('vars', networks_lower[network] ~ '_cidr') }}
              routes: {{ lookup('vars', networks_lower[network] ~ '_host_routes') }}
          {% endfor %}

        # These vars are for the network config templates themselves and are
        # considered EDPM network defaults.
        neutron_physical_bridge_name: br-ex
        neutron_public_interface_name: eth0
        # edpm_nodes_validation
        edpm_nodes_validation_validate_controllers_icmp: false
        edpm_nodes_validation_validate_gateway_icmp: false
        edpm_network_config_nmstate: false
        edpm_network_config_update: false
        dns_search_domains: []
        gather_facts: false
        # edpm firewall, change the allowed CIDR if needed
        edpm_sshd_configure_firewall: true
        edpm_sshd_allowed_ranges:
          - 192.168.122.0/24
    networks:  # mapped to spec.nodeTemplate.networks
      - defaultRoute: true
        name: ctlplane
        subnetName: subnet1
      - name: internalapi
        subnetName: subnet1
      - name: storage
        subnetName: subnet1
      - name: tenant
        subnetName: subnet1
    nodes:  # mapped to spec.nodes
      edpm-compute-0:
        hostName: edpm-compute-0
        bmhLabelSelector:
          nodeName: edpm-compute-0
        networkData:
          name: edpm-compute-0-network-data
          namespace: openstack
        # must be defined if custom networkData is used for BMO, and there is a static IP defined for ctplane interface
        networks:
          - name: ctlplane
            subnetName: subnet1
            defaultRoute: true
            fixedIP: 192.168.122.100
          - name: internalapi
            subnetName: subnet1
          - name: storage
            subnetName: subnet1
          - name: tenant
            subnetName: subnet1
    services:
      - bootstrap
      - download-cache
      - configure-network
      - validate-network
      - install-os
      - configure-os
      - ssh-known-hosts
      - run-os
      - reboot-os
      - install-certs
      - libvirt
      - ovn
      - neutron-ovn
      - nova-custom-gpu
      - neutron-metadata
      - vfio-pci-bind
      - telemetry
  nova:
    # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to cpu-pinning-nova ConfigMap
    compute:
      conf: |
        # CHANGEME
        [DEFAULT]
        reserved_host_memory_mb = 4096
        reserved_huge_pages = node:0,size:4,count:524160
        reserved_huge_pages = node:1,size:4,count:524160
        [compute]
        cpu_shared_set = 0-3,24-27
        cpu_dedicated_set = 8-23,32-47
    # Mapped in lib/dataplane/nodeset/kustomization.yaml to the nova-migration-ssh-key secret
    migration:
      ssh_keys:
        private: CHANGEME4
        public: CHANGEME5
    # Mapped in va/nvidia-vfio-passthrough/edpm/nodeset/kustomization.yaml to gpu-nova ConfigMap
    pci:
      # You will have one device_spec line for each physical GPU you want to make available for passthrough:
      # "vendor_id": The vendor ID of the GPU manufacturer (e.g., "10de" for NVIDIA).
      # "product_id": The product ID of the specific GPU model (e.g., "20f1" for an NVIDIA A100).
      # "address": The PCI address of the GPU on the host machine. You can find this using `lspci | grep -i nvidia`.
      # "physical_network": This is used for SR-IOV networking passthrough. For a GPU, you can typically leave this as null.
      # "alias": must match the alias configuration in the API service-values.yaml
      conf: |
        # CHANGEME
        [pci]
        device_spec = {"vendor_id":"10de", "product_id":"20f1", "address": "CHANGEME", "physical_network":null}
        alias = { "vendor_id":"10de", "product_id":"20f1", "device_type":"type-PCI", "name":"nvidia_a2" }
        report_in_placement = True
